1.spark简介
	Spark是一个用来实现 [快速] 而 [通用]的集群计算的平台。
	扩展了广泛使用的MapReduce计算模型，而且高效地支持更多的计算模式，包括交互式查询和流处理。
	在处理大规模数据集的时候，速度是非常重要的；Spark的一个重要特点就是能够在内存中计算，因而更快。
	即使在磁盘上进行的复杂计算，Spark依然比MapReduce更加高效。
2.spark的特点
	速度快：
		扩充了流行的MapReduce计算模型 [mapreduce在迭代式计算和交互式上低效]
		Spark是基于内存的计算
	通用性： [应用场景广泛]
		容纳了其他分布式系统拥有的功能
		批处理，迭代式计算，交互查询和流处理等
		优点：降低了集群的维护成本
	高度开放：
		提供了Python，java，Scala，SQL的API和丰富的内置库
		和其他的大数据工具整合的很好，包括Hadoop，kafka等
3.spark vs Hadoop
	Hadoop应用场景：
		----> 计算时间一般是几分钟到几小时
		离线处理
		对实时性要求不高
	Spark应用场景：
		----> 计算时间一般是几秒钟到几分钟
		时效性要求高的场景
		机器学习的领域
	Spark不具有hdfs的存储能力，要借助hdfs等持久化数据
		
4.spark集群搭建 ---- standalone模式
	
	https://blog.csdn.net/u011563666/article/details/79298314 [yarn模式搭建集群]
	spark集群启动流程：
		启动master进程
		Master进程启动完成后，会解析slaves配置文件，找到启动Worker的host，然后启动相应的Worker，并发送注册信息给Worker
		worker向master注册
		Master收到注册信息后，并保存到内存和磁盘里；Master给Worker发送注册成功的消息（MasterURL）
		Worker收到Master的URL信息后，开始与Master建立心跳

	spark的shell：
		Spark的shell能够处理分布在集群上的数据
		Spark把数据加载到节点的内存中，因此分布式处理可以在秒级完成
		快速迭代式计算，实时查询，分析一般能够在shell中完成
	spark组件：
		Apache Spark Core:包含spark的基本功能，包含任务调度，内存管理，容错机制等；内部定义了RDD；
			应用场景：为其他组件提供底层的服务
		Spark SQL：是spark处理结构化数据的库，就像hive sql，mysql一样
			应用场景：在企业中做报表统计
		spark streaming：实时数据流处理组件，类似Storm
			应用场景：企业中用来从kafka接收数据做实时统计
		MLib：一个包含通用机器学习功能的包，包含分类，聚类，回归等，还包括模型评估，和数据导入；支持集群的横向扩展
			应用场景：机器学习
		GraphX：是处理图的库，并进行图的并行计算，提供了各种图的操作，和常用的图算法
			应用场景：图计算

	spark紧密集成的优点：
		Spark底层优化了，基于Spark底层的组件，也会得到相应的优化
		紧密集成，节省了各个组件使用时的部署，测试等时间
		向Spark增加新的组件时，其他组件，可立刻共享新组件的功能

5.spark集群的任务提交执行流程
	spark on standalone
		用户通过spark-submit脚本提交应用，Driver使用本地的Client类的main函数来创建SparkContext对象并初始化
		SparkContext连接到Master，注册并申请资源（内核和内存）
		Master收到任务信息后，开始资源调度，和所有的Worker进行通信，找到空闲的Worker，通知Worker启动Executor子进程
		Executor进程启动后，开始与Driver通信，向SC注册，之后SparkContext将应用程序代码发送到各Executor，最后将任务
			（Task）分配给Executor执行
		Executor创建Executor线程池，开始执行task，并向SC汇报Task状态直到结束
		所有task运行完成后，SparkContext向Master注销，释放资源

	· ClusterManager：集群管理器，控制整个集群，监控Worker，此处为Master
	· Worker：从节点，负责控制计算节点，启动Executor
	· Driver：运行Application的main（）函数，创建SparkContext对象
	· Executor：执行器（进程），在Worker上执行任务的组件，用于启动线程池运行任务。每个Application拥有独立的一组Executor
	· SparkContext[DAGScheduler、TaskScheduler]：整个应用的上下文，控制应用的生命周期
	· RDD：Spark的基本数据结构
	· DAGScheduler：根据作业（Job）构建基于Stage的DAG，并提交Stage给TaskScheduler
	· TaskScheduler：将stage中的任务（Task）分发给Executor
	· SparkEnv：线程级别的上下文，存储运行时的重要组件的引用

	注意：
		一个worker可以启动多个Executor进程，执行来自不同应用程序的任务
		spark-shell模式启动有两种模式：集群模式和本地模式
		如果提交到本地，则不会启动Executor进程

	==================================================================================================
	开发第一个Spark程序
	本机配置免密登录可以采用如下操作
	ssh-keygen
	touch authorized_kyes
	cat id_rsa.pub > authorized_kyes
	chmod 600 authorized_kyes
	ssh localhost
	wordCount
		创建一个SC
		加载数据
		把每一行分割成单词
		转换成pairs并且计数
	打成jar包提交到集群上
		启动集群
		提交作业
			/bin/spark-submit --master spark://hadoop01:7077 --class className jar_Path
	==================================================================================================
=================================================================================================================
Driver program
	包含程序的main（）方法，RDDS的定义和操作，程序的入口
	管理很多节点，称作executors
SparkContext
	Driver program通过SparkContext对象访问Spark
	SparkContext对象代表和一个集群的连接
	在shell中SparkContext是自动从创建好的，就是sc

=================================================================================================================
6.RDD [弹性分布式数据集]
	===============================================================================
	RDD是spark分发数据和计算的基础抽象类
	这些RDD，并行的分布在整个集群中
	例如：lines即一个RDD
	#无论words这个文件有多大，lines始终代表整个文件
	#大文件可能分发在多个节点上，但是由一个整体变量lines来进行操作
	val lines = sc.textFile("/usr/local/hadoopfiles/words") 
	lines.count
	# res4: Long = 8 结果是文件中的总行数
	一个RDD是一个不可改变的分布式集合对象
	例如：
	从文本文件加载数据之后，lines这个RDD就不能再改变了,因为不可能对源文件进行增删改
	Spark中，所有的计算都是通过RDD的创建，转换，操作完成的
	一个RDD内部由许多partitions（分片）组成
	===============================================================================
	每个分片包括一部分数据，partitions可以在不同节点上计算
	分片是Spark并行处理的单元，Spark顺序的，并行的处理分片
	===============================================================================
	scala基础知识：
	在scala中创建变量的时候，必须使用val或者var
		val，变量值不可修改，一旦分配不能重新指向别的值
		var，分配后，可以指向类型相同的值
	scala匿名函数和类型推断
	lines.filter(line => line.contains("world")) 
		定义了一个匿名函数，接收一个参数line
		使用line这个String类型变量上的contains方法，并且返回结果
		line的类型不需要指定，能够推断出来
	===============================================================================
	基本操作：
	Transformation
	从之前的RDD转换成一个新的RDD
	Action
	在RDD上计算出来一个结果
	把结果返回给Driver program或保存在文件系统中
	===============================================================================
	特性：
	RDD的血统关系图
		Spark维护RDD之间的依赖关系和创建关系，叫做血统关系图
		Spark使用血统关系图来计算每个RDD的需求和恢复丢失的数据
	延迟计算
		Spark对RDD的计算是，他们第一次使用action操作的时候
		在大数据处理中，可以减少数据传输
		Spark内部记录metadata表明transformations操作已经被响应了
		加载数据也是延迟计算，数据只有在必要的时候，才会被加载进去
	RDD.persist 持久化
		默认每次在RDD上面进行action操作时，Spark都会重新计算RDD
		如果想重复利用一个RDD，可以使用RDD.persist\(\)
		unpersist\(\)方法从缓存中移除
	===============================================================================


=================================================================================================================
对RDD的认识：	
	· 不可变
		RDD是一个只读的、分区记录的集合。通过三种方式可以创建RDD
	· 可分区
		一个RDD可以包含多个分区，每个分区就是一个dataset片段
	· 元素可并行计算
		元素来自跨节点的数据集，可以并行处理这些数据集 [每个RDD的数据都以block的形式存储于多台机器上]
		 
	
	可以理解为RDD是一个提供了许多操作接口的数据集合，RDD基础操作和scala集合的操作非常类似，分为转化操作和行为操作；转化操作就是
		从一个RDD产生一个新的RDD，而行动操作就是进行实际的运算
=================================================================================================================

	弹性分布式数据集
		
		弹性：
			可以存储任意类型的数据
				RDD其实不存储真实的数据,只存储数据的获取方法,以及分区的方法，还有就是数据的类型
			对数据的操作可以基于内存可以基于磁盘
			RDD可以转换成其他的RDD
		
		分布式：
			来自跨节点的数据集，可以并行处理这些数据集

		缓存可以提高执行效率

		容错性能

	RDD其实是不存储真实数据的，存储的只是真实数据的 [分区信息] getPartitions，还有就是针对单个分区的[读取方法]compute
	RDD存储：
		初代RDD：处于血统的顶层，存储的是任务所需的数据的分区信息，还有单个分区数据读取的方法，没有依赖的RDD，因为它就是依赖的开始
		子代RDD：处于血统的下层，存储的东西就是初代RDD到底干了什么才会产生自己，还有就是初代RDD的引用
	数据读取：
		数据读取是发生在运行的Task中，也就是说，数据在任务分发的Executor上运行的时候读取的
		在spark中有两种Task，一种是ResultTask，一种是ShuffleTask，两种Task都是以相同的方式读取RDD的数据
	
	创建方式：
		
		starting with a file in the Hadoop file system(or any other Hadoop-supported file system)
			val rdd2 = sc.textFile("hdfs://hadoop01:9000/wordcount/input/words") #可以是外部文件系统中的文件或者本地文件
		existing Scala collection in the driver program
			 集合并行化创建（通过scala集合创建）scala中的本地集合 ----> spark RDD
			#第一个参数：待并行化处理的集合 第二个参数：分区个数
			val rdd = sc.parallelize(Array(1,2,3),4) 
		transforming it [对已有的RDD通过方法转换成新的RDD]
			val rdd1 = rdd.map(_*10)
	
	运行逻辑：
		在Spark应用中，整个执行流程在逻辑上运算之间会形成有向无环图。驱动器执行时，它会把这个逻辑图转换为物理执行计划，然
			后将逻辑计划转换为一系列的步骤（stage），每个步骤有多个任务组成
		Spark的调度方式与MapReduce有所不同。Spark根据RDD之间不同的依赖关系形成不同的阶段（Stage），一个阶段包含一系列函
			数进行流水线执行
	属性：
		一组分片：即数据集的基本组成单位，每个分片会被一个计算任务处理，并决定并行计算的粒度
		基于每一个分片做计算
		RDD之间的依赖关系
		分片函数 [基于哈希的HashPartitioner，另一个是基于范围的RangePartitioner]
		一个列表，存储存取每个Partition的优先位置 [移动数据不如移动计算]
			[理解：RDD的数据分布式存储在多台机器节点上，spark进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据
			块的存储位置]
	==========================================================================================
	对RDD分区的认识：
		分区实际上就是RDD数据集的一个片段，对RDD进行操作，实际上是操作RDD中的每一个分区
		分区的数量决定了并行的数量，RDD基于每一个分区计算，所以RDD的分区数目决定了总的Task数目
		
		计算逻辑：
			getPartitions获取分区
			getSplits
			goalSize [totalSize / numSplits]
			blockSize
			minSize
		分区的个数：

		一个分片一定是来自某一个文件
		分片不会跨RDD
	==========================================================================================
	创建分区的意义：
		增加并行处理能力
		决定了shuffle输出的分片数量
	分片的大小如何计算：
		goalSize(totalSize,numSplits),blockSize,minSize

7.RDD Operations
https://blog.csdn.net/dream0352/article/details/62229977 [spark常用算子讲解]
	RDD支持两种类型的操作：
		transformation [转换算子]：从已经存在的数据集创建一个新的数据集
			操作是延迟计算的，需要等到Action操作的时候才会真正触发运算
			这种设计也让spark更加有效率的执行
			数据项包括
				Value型数据 
				Key-Value型的数据对
			（1）map算子
				将原来RDD的每个数据项通过map中的用户自定义函数f映射转变为一个新的元素
			（2）flatMap算子
				将原来RDD中的每个元素通过函数f转换为新的元素，并将生成的RDD的每个集合中的元素合并为一个集合
				对每个输入元素，输出多个输出元素 [成行元素会被打散]
				flat压扁的意思，将RDD中元素压扁后返回一个新的RDD
			（3）mapPartitions算子
				mapPartitions函数获取到每个分区的迭代器，在函数中通过这个分区整体的迭代器对整个分区的元素进行操作
			（4）集合运算
				union算子 [并集]
				使用union函数时需要保证两个RDD元素的数据类型相同，返回的RDD数据类型和被合并的RDD元素数据类型相同，
					并不进行去重操作，保存所有元素
					去重可以使用distinct（）
				intersection算子 [交集]
				val rdd_inter = rdd1.intersection(rdd2)
				subtract算子 [差集]
				val rdd_sub = rdd1.subtract(rdd2)
			（5）Key-Value型的数据对的常用算子
				groupByKey算子 [把相同key的values分组]
				将元素通过函数生成相应的key，数据转化为Key-Value格式，之后将key相同的元素分为一组
				用户函数预处理 ----> map操作 ----> groupByKey分组操作
				分区器：HashPartitioner & RangePartitioner
				尽量避免使用此算子，map端没有局部合并，对shuffle时的网络传输压力较大

				reduceByKey算子 [把相同key的结合]
				从功能上讲相当于先做GroupByKey然后再做reduce操作，但是操作效率要比gruopByKey+reduce高
				包含shuffle操作
				效率要高是因为reduceByKey在map端自带Combiner
				分组 ----> 局部聚合 ----> shuffle ----> 全局聚合
				
				combineByKey算子 [把相同key的结合，使用不同的返回类型]
				----> 最常用的基于key的聚合函数，返回的类型可以与输入的类型不一样
				createCombiner：V=>C1,以当前values作为参数，对其进行类型转换并返回 [类似于初始化操作]
				mergeValue:(C1,V) => C2,把元素V合并到之前的元素C1上 [在每个分区内进行]
				mergeCombiners:(C2,C2) => C3,把两个元素C2合并 [在不同分区间进行]
					----> 这里的C2，第一个参数表示前几个partition已经累计计算到的值，第二个参数表示新的partition的值
				遍历partition中的元素，元素的key，要么之前见过的，要么不是
				如果是新元素，使用我们提供的createCombiner函数
				如果是这个partition中已经存在的key，就会使用mergeValue函数
				合计每个partition的结果的时候，使用mergeCombiners函数

				例子： 求平均值
				val scores = sc.parallelize(Array(("jake",90.0),("tom",80.0),("jake",77.0),("tom",67.0),("jake",89.0),("tom",97.0)))
				scores.foreach(println) #打印结果
				#基于key对values进行操作
				#得到科目数和总分
				val score_new = scores.combineByKey(score => (1,score),(c1:(Int,Double),newScore) => (c1._1+1,c1._2+newScore),(c1:(Int,Double),c2:(Int,Double)) => (c1._1 + c2._1,c1._2 + c2._2))
				#Int和Double类型的值相加也会报错，所以如果指定成绩是Double类型的值就保留到几位小数
				#求分平均分
				val average_score = score_new.map{case(name,(num,totalScore)) => (name,totalScore/num)}
				average_score.foreach(println)
				mapValues算子
				函数作用于pairRDD的每个元素，key不变，对values进行操作
				keys算子
				仅返回keys
				values算子
				仅返回values
				sortByKey算子
				按照key排序的RDD
			（6）filter算子
				对元素进行过滤
			（7）distinct算子
				去重
			（8）sample算子
				将RDD集合中的元素进行采样，获取所有元素的子集
				参数设置：是否有放回的抽样、百分比，随机种子
				随机种子一样，产生的结果一样 [伪随机]
			（9）mapValues算子
				对Key-Value型数据进行map操作，不对key进行处理
			（10）partitionBy算子
				对RDD进行分区操作
			（11）join算子
				本质是通过cogroup算子先进行协同划分，再通过flatMapValues将合并的数据打散
			（12）repartition算子
				默认shuffle操作为true
				reparation就是coalesce的shuffle参数为true的实现
			（13）coalesce算子
				减少分区优先使用coalesce算子，避免shuffle发生，因为分区的合并不需要进行网络传输
				如果重分区的数目大于原来的分区数，那么必须指定shuffle参数为true，否则，分区数不变
		actions [行动算子]：在数据集中运行计算之后，返回一个值给驱动程序
			触发SparkContext提交job作业，并将数据输出Spark系统
			一段spark代码里至少要有一个action操作

			（1）foreach算子
				对RDD中的每个元素都应用f函数操作，不返回RDD和Array，而是返回Unit
				如果是print函数，那么打印的结果可能不一样，因为存在多个分区的情况，先打印哪个分区的数据是随机的
				不返回到本地
			（2）saveAsTextFile算子
				函数将数据输出，存储到HDFS的指定目录或本地磁盘上
			（3）collect算子
				将分布式的RDD返回为一个单机的scala Array数组。在这个数组上运用scala的函数式操作
				将计算的结果汇总到Driver端，数据量比较少的情况下使用
				需要单机内存能够容纳下（因为数据要拷贝给driver，测试使用）
			（4）count算子
				返回整个RDD的元素个数
			（5）fold算子
				初始值在局部聚合和全局聚合的时候都会使用
			（6）aggregate算子
				与transformation算子类似，但是初始值在全局聚合的时候也会起作用
				如果初始值是字符串类型，则是字符串的拼接操作，同样是局部聚合和全局聚合都会连接初始值
			（7）take算子
				作用与collect算子类似，返回前N个元素
			（8）takeSample算子

			（9）takeOrdered算子
				返回升序排序的前N个元素
			（10）top算子
				返回降序排序的前N个元素
			（11）take算子
				返回RDD的n个元素 [同时尝试访问最少的partitions]
				返回结果是无序的，测试使用
				随机取n个
			（12）first算子
				返回第一个元素
			（13）reduce算子
				接收一个函数，作用在RDD两个类型相同的元素上，返回新元素
				可以实现，RDD中元素的累加，计数，和其他类型的聚集操作
	所有的transformation只有遇到action才能执行
	当触发执行action之后，数据类型就不在是RDD了，数据就会存到指定的文件系统中，或者直接打印结果或者收集起来


reduceByKey 和 groupByKey 和CombineByKey对比分析
    CombineByKey:有类型转换的功能
===============================================================================================================
对spark整体流程的理解：
	驱动程序就是执行了一个Spark Application的main函数和创建SC的进程，包含了这个Application的全部代码
	Spark Application中的每个action会被Spark作为job进行调度
	每个job是一个计算序列的最终结果，而这个序列中能够产生中间结果的计算就是一个stage
	Action对应了job，而Transformation对应了stage
	发生shuffle可以改变分区的个数
===============================================================================================================
理解：
map、mapPartitions、flatMap、MapPartitionsWithIndex区别对比分析       
	map：将原来RDD的每个数据项通过map中的用户自定义函数f映射转变为一个新的元素
	mapPartitions：mapPartitions函数获取到每个分区的迭代器，在函数中通过这个分区整体的迭代器对整个分区的元素进行操作
		即先partition，再对每个partition进行map函数0
	flatMap：将原来RDD中的每个元素通过函数f转换为新的元素，并将生成的RDD的每个集合中的元素合并为一个集合
	mapPartitionWithIndex：与mapPartitions类似，按照分区进行map操作，但是多传入了一个参数：当前处理分区的index
===============================================================================================================
理解：
Spark 重分区函数：coalesce和repartition区别与实现，可以优化Spark程序性能！！
    *coalesce：
    返回一个经过简化到numPartitions个分区的新RDD。这会导致一个窄依赖，
    例如：你将1000个分区转换成100个分区，这个过程不会发生shuffle，相反如果10个分区转换成100个分区将会发生shuffle。
    然而如果你想大幅度合并分区，例如合并成一个分区，这会导致你的计算在少数几个集群节点上计算（言外之意：并行度不够）。
    为了避免这种情况，你可以将第二个shuffle参数传递一个true，这样会在重新分区过程中多一步shuffle，这意味着上游的分区可以并行运行。
  　
        ~注意：第二个参数shuffle=true，将会产生多于之前的分区数目,
         例如你有一个个数较少的分区，假如是100，
         调用coalesce(1000, shuffle = true)将会使用一个  HashPartitioner产生1000个分区分布在集群节点上。
         这个（对于提高并行度）是非常有用的。

  　*repartition：
    返回一个恰好有numPartitions个分区的RDD，可以增加或者减少此RDD的并行度。内部，
    这将使用shuffle重新分布数据，如果你减少分区数，考虑使用coalesce，这样可以避免执行shuffle

    两者的关系：看下源代码的调用
    如果想要减少分区用：coalesce
===============================================================================================================

    sortBykey与sortBy区别
        前者：键值对
===============================================================================================================
    collect 是从worker收集数据到driver,过大可以保存到磁盘

    count控制RDD有多少元素的个数
    countbykey　是相同key的个数

    foreach是在每个task上的操作，他的结果是在每个worker

===============================================================================================================
   rdd统计Action
   .sum()
   .min
   .max
   .mean均值
   .variace方差：数据波动
   .stdev()标准差

   * 函数的来源：
    单独的函数
    每个对象的方法也可以传入参数,当做函数
    函数的执行:
    在worker
    *网络传输的算子都是序列化的
    *调用算子的时候，对当前的RDD是否适用
===============================================================================================================
     wordCount处理流程：
     开始　一共有６个RDD（面试题）
     将文件转换成HadoopRdd(k,v)并分区　 k是偏移量，是内容
     然后进行一次map得到一个RDD[String]
     flatMap
     reducebykey,触发shuffle和hadoop的shuffle一样
     然后保存文件　savaasFile 会调用mappartiton将数据输出

===============================================================================================================
    练习题：topN
    三种解决方案：
        *第一种： 详看work01
===============================================================================================================
    RDD的依赖关系
    窄依赖类型有两个：oneToOneDependency和RangeDependency
    如果发生ShuffleDependency肯定是宽依赖

   窄依赖：有一个父分区　对应一个子分区，
          一个子分区对应多个父分区

   (体现出了容错)
   one to one Dependency的getParents 可以得到当前子分区的所有父分区ID　
        根据算子的操作可以恢复当前算子(体现出了容错)
   (如果基于范围分区)RangeDependency的getParents
 详解:https://blog.csdn.net/u011564172/article/details/54312200

  宽依赖　ShuffleDependency


  　 窄依赖与宽依赖如何产生的？

    spark容错机制讲一下？

        *１、依赖关系的设置
        Spark的这种依赖关系设计，使其具有了天生的容错性，大大加快了Spark的执行速度。
    因为，RDD数据集通过“血缘关系”记住了它是如何从其它RDD中演变过来的，血缘关系记录的是粗颗粒度的转换操作行为，
    当这个RDD的部分分区数据丢失时，它可以通过血缘关系获取足够的信息来重新运算和恢复丢失的数据分区，
    由此带来了性能的提升。相对而言，在两种依赖关系中，窄依赖的失败恢复更为高效，
    它只需要根据父RDD分区重新计算丢失的分区即可（不需要重新计算所有分区），
    而且可以并行地在不同节点进行重新计算。而对于宽依赖而言，单个节点失效通常意味着重新计算过程会涉及多个父RDD分区，
    开销较大。
        *２、Spark还提供了数据检查点和记录日志，
        用于持久化中间RDD，从而使得在进行失败恢复时不需要追溯到最开始的阶段。在进行故障恢复时，
         Spark会对数据检查点开销和重新计算RDD分区的开销进行比较，从而自动选择最优的恢复策略。
        *3 https://blog.csdn.net/dengxing1234/article/details/73613484

   如何知道父分区具体到子分区？
        答：通过分区器,比如HashPartition,rangePartition
   扩充：Hashpartition与RangePartition如何分区，并且有什么缺点？

   那些是宽依赖和窄依赖？
   发生shuffle的,有bykey的,join基本都是宽依赖
   依赖关系都和什么有关系？

   容错都有哪几种？　
   一个是RDD，血统解决
   一个是集群，资源调度解决

   如何查看依赖关系？
        从两个方面：　
        １、toDebugString()方法
        ２、dependencies方法
    血统与saprk高效的关系？（待百度）
        答：１、在mapreduce中他会每一步都落地到磁盘，去做计算
            在spark中会遇到shuffleRDD才可能会落地到磁盘，其他会在内存中计算
            ２、每一步不需要序列化的，mapreduce每一步落地当磁盘都需要序列化
            ３、RDD是只读的


   每个RDD都记录着依赖，通过getPartition得到父分区的id
===============================================================================================================
10.Spark为什么比mapreduce快？
    答：1）基于内存计算，减少低效的磁盘交互；
        2）高效的调度算法，基于DAG；
        3)容错机制Linage，精华部分就是DAG和Lingae
===============================================================================================================
11、RDD血统的理解
    RDD数据集通过所谓的血统关系(Lineage)记住了它是如何从其它RDD中演变过来的。
    血统记录的是粗颗粒度的特定数据转换（Transformation）操作（filter, map, join etc.)行为。
    当这个RDD的部分分区数据丢失时，
    它可以通过Lineage获取足够的信息来重新运算和恢复丢失的数据分区。
    这种粗颗粒的数据模型，限制了Spark的运用场合，但同时相比细颗粒度的数据模型，也带来了性能的提升。

===============================================================================================================
12、 RDD的容错是如何实现的

    在RDD计算，通过checkpoint进行容错，
    做checkpoint有两种方式，
    一个是checkpoint data，
  　 一个是logging the updates。
    用户可以控制采用哪种方式来实现容错，默认是logging the updates方式，
    通过记录跟踪所有生成RDD的转换（transformations）
    也就是记录每个RDD的lineage（血统）来重新计算生成丢失的分区数据。

    RDD数据集通过“血缘关系”记住了它是如何从其它RDD中演变过来的，血缘关系记录的是粗颗粒度的转换操作行为，
    当这个RDD的部分分区数据丢失时，它可以通过血缘关系获取足够的信息来重新运算和恢复丢失的数据分区，
    由此带来了性能的提升。相对而言，在两种依赖关系中，窄依赖的失败恢复更为高效，
    它只需要根据父RDD分区重新计算丢失的分区即可（不需要重新计算所有分区），
    而且可以并行地在不同节点进行重新计算。而对于宽依赖而言，单个节点失效通常意味着重新计算过程会涉及多个父RDD分区，
    开销较大。

===============================================================================================================
　Spark　实现 缓存
    缓存是SPARK构建迭代式算法和快速交互式查询的关键,cache 机制保证了需要访问重复数据的应用（如迭代型算法和交互式应用）可以运行的更快

    有两种方法　一、cache 二、persist
    *两者区别与联系？
    cache和persist都是用于将一个RDD进行缓存的.这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间。
    cache调用的是persist（memory_only）
     *十二种级别,所以persist(MEMORY_ONLY)，进行设置
         val MEMORY_ONLY = new StorageLevel(false, true, false, true)
         useDisk：使用硬盘（外存）
         useMemory：使用内存
         useOffHeap：使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。
         deserialized：反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象
         replication：备份数（在多个节点上备份）

    *什么时候去缓存?
        1、发生shuffle
        2、算子被重复利用
        ３、rdd计算过长
    *缓存的机制
    *缓存级别怎么去用？优先性？　
    *缓存的时候为什么进行序列化? https://blog.csdn.net/junerli/article/details/78729006
    *缓存怎么释放：reduce.unpersist()
    *LRU算法释放缓存？最近最少使用算法 LRU
    *缓存的内容在哪里?　在worker
    * 缓存不够用怎么办？　可以到磁盘，也可以加大内存
    *                       ```如何考虑缓存的存储级别?　第一、如果内存够大直接存，第二如果内存够呛，就序列化与反序列化到内存,以时间换取空间，如果反复使用的算子可能会快点，第三、到磁盘
    *存在缓存的故障怎么办？就要使用带有备份的缓存。
    *如果缓存丢失了怎么去恢复？血统

    Spark cache的用法及其误区分析
    https://www.imooc.com/article/35448

    *存储级别没有被序列化，则文件会变大，因为当前RDD存储的是java对象，所以导致文件变大



    什么时候使用checkpoint?　
                            １、如果RDD之间的依赖（血统）过长　
                            ２、shuffle之后的使用checkpoint，因为要进行网络传输
    checkpoint的的读写流程？
    checkpoint过程？
        １、sc.setCheckpointDir("hdfs的目录")
        2、val tuples = lines.map(x=>((x.split(",")(1),x.split(",")(2).toInt)) //得到数字
        ３、val reduced = tuples.reduceBykey(_+_)
        4、reduced.Checkpointed
        ５、reduced.isCheckpointed //此时显示false 因为他是transformtion算子
        ６、reduced.count  //执行一个action 算子
        ７、reduced.isCheckpointed //此时显示true,就能从文件中查看

===============================================================================================================
    cache与persisit与checkpoint区别？
    安全性方面
    https://blog.csdn.net/ImBetter/article/details/80023338
    checkpoint什么时候去使用？
        RDD的第五个属性，获取列表，选取最优位置 优先选取checkpoint
===============================================================================================================
    spark性能调优的战略?
   note:最佳性能的是通过业务场景、以及数据情况，以及对spark作业进行综合性的分析
     组成部分：开发调优，资源调优，数据倾斜调优，shuffle调优、并行度调优、序列化调优
     １、操作并行度调优：　
     当RDD的分区数据过大，分区太多，还有就是分区没数据（比如filter后的　）
                        第一种在数据混合时指定RDD
                       第二种　使用repartition（打乱分区） 与coalesce（减少分区、不打乱分区高效）
     ２、序列化
     数据在网络传输或者溢写到磁盘时如果直接写会导致文件过大，所有序列化操作。以二进制传输存储，提供两种一个是java提供的序列化，还有就是kryo

===============================================================================================================
    DAG的是生成
        在Action算子之前就生成一个有向无环图
        repalication会包换多个DAG
     RDD与DAG的区别
        DAG是整个RDD的依赖关系
        RDD是


        DAG -> DAGScheduler(对DAG切分为不同的stage，切分标准：是否发生shuffle(宽依赖),１个shuffle－> stage->task) -> task集合
        以WC为｀例，从后（saveASFile）往前遇到shuffle宽依赖）就切分为一个，直到DAG的开头，在从头执行，一个个stage执行
        每个stage，根据RDD（此时都是窄依赖）的分区个数，生成task.
        此时会是一套并行流水线，每个分区就是一条流水线，就是一个task,所以说task在这个stage中是跨分区的
        结果每个stage会有多个task,通过taskSet发送给TaskScheduler然后  分发给worker


        注意发生groupbykey（只要调用都是宽依赖）与join的宽窄依赖根据RDD的有关~~//待百度
===============================================================================================================

        默写内容：
        1，spark任务提交的四个阶段，
        答：
        1，构建DAG
        DAG描述多个RDD的转换过程，任务执行时，可以按照DAG的描述，执行真正的计算；
        DAG是有边界的：开始（通过sparkcontext创建的RDD），
        结束（触发action，调用runjob就是一个完整的DAG形 成了，‘一旦触发action，
        就形成了一个完整的DAG）；
    　　　一个spark application可以包含多个DAG，
        取决于具体有多少个action。
        2，DAGScheduler将DAG切分stage（切分依据是shuﬄe）,结果taskSet
        逆向切分，正向提交　结果
        将stage中生成的task以taskset的形式发送给 TaskScheduler
        为什么要切分stage？
        一个复杂是业务逻辑（将多台机器上具有相同属性的数据聚合到一台机器上:shuﬄe）
        如果有shuﬄe，那么就意味着前面阶段产生结果后，
        才能执行下一个阶段，下一个阶段的计算依赖上一个阶段的数据
        在同一个stage中，会有多个算子，可以合并到一起，
        我们称其为pipeline（流水线，严格按照流程、顺序执行）

        3，TaskScheduler 调度task（根据资源情况将task调度到Executors）

        4，Executors接收task，然后将task交给线程池执行



        2，描述stage划分过程，
         看附件　stage的划分

        3，广播变量、累加器在什么情况下使用



        广播变量（Broadcast）
        1、能不能将一个RDD使用广播变量广播出去？

               不能，因为RDD是不存储数据的。可以将RDD的结果广播出去。

        2、 广播变量只能在Driver端定义，不能在Executor端定义。

        3、 在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值。

        4、如果executor端用到了Driver的变量，如果不使用广播变量在Executor有多少task就有多少Driver端的变量副本。

        5、如果Executor端用到了Driver的变量，如果使用广播变量在每个Executor中只有一份Driver端的变量副本。

        只在 worker 存一份 ，多个 task 共享，不会为每个task都拷贝一份。
        目的：减少变量到各个节点的网络传输消耗，以及在各个节点上的内存消耗
        缺点：是只读的，不可修改

        累加器（Accumulate）
        在driver端定义，只能在exector更新
        Accumulator，主要用于多个节点对一个变量进行共享性的操作（累加操作）。
        特性：
        累计器只能在Driver定义初始化。
        task只能对Accumulator进行累加操作，不能读取它的值。

        4，什么情况下需要自定义排序方法，如何使用
            当对一个数据类型进行排序时有两个以上的排序规则是使用自定义排序
            有五种
              * 方案一、用类或者样例类来封装数据
              把数据封装成类或者case class，然后类继承Ordered[类型] ，然后可以自定义排序规则。

              如果是class，需要实现序列化特质，Serializable,
            　如果是case class，可以不实现该序列化特质。
              这种处理方式，返回值类型是类的实例对象。
              方案二
               * 利用class或者case class指定排序规则
                对原始数据不进行封装，仅仅在排序的时候，利用class或者case class指定排序的规则。
                如果使用类，需要继承Ordered[类型]，实现序列化特质，
                如果使用case class，不需实现序列化特质。
                返回值的结果类型：还是原来的数据类型。和类本身无关。仅仅是利用类的规则来实现了排序。
                方案三
                  * 利用隐式转换时，类可以不实现Ordered的特质，普通的类或者普通的样例类即可。
                隐式转换支持，隐式方法，隐式函数，隐式的object和隐式的变量，
                如果都同时存在，优先使用隐式的object，隐式方法和隐式函数中，会优先使用隐式函数。
                隐式转换可以写在任意地方（当前对象中，外部的类中，外部的对象中），如果写在外部，需要导入到当前的对象中即可。
                  */
                  方案四
                   最简单的实现方案，直接利用元组来封装要排序的条件，默认升序，降序使用-号即可
===============================================================================================================
sparkshuffle
https://blog.csdn.net/zhanglh046/article/details/78360762

        

            
















